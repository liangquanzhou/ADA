ADA    HW5 
------------------------------------------
Minqian Guo(mg3418)
------------------------------------------

**Prob a. Fit a multiple linear regression model**

```{r}
require(MASS)
data <- Pima.te
fit <- lm(glu~npreg+bp+skin+bmi+age,data=data)
summary(fit)
```

**Prob b. The underlying assumptions check**

1. Nonlinearity: Using R-squared value in the result of linear regression.

```{r}
summary(fit)$r.squared
```
Since the R-squared is small, which means the lack of fit of fitted model.


2. Normality 
```{r}
qqnorm(fit$residuals)
qqline(fit$residuals)
```

The QQ plot shows the assumption of normality is invalid. 

```{r}
shapiro.test(fit$residuals)
shapiro.test(fit$residuals)$p.value
```
By using Shapiro-Wilk test to check normality, Since the p-value  is significantly small, we reject the null hypothesis that the distribution of the error is not normal.

3. Homoscedasticity 

```{r}
plot(fit$fitted.values,fit$residuals,xlab='Fitted Value',ylab='Residual',main="Resdual Plot against Fitted Value")
abline(h=0)
```

The plot of the residuals against the fitted values shows a scatter plot without certian pattern. Thus the assumption of constant error is valid.

4. Uncorrelation of Error
```{r}
require(lmtest)
dwtest(fit)
dwtest(fit)$p.value
```
By using Durbin-watson test to do correlation check of error, Since p-value is greater than 0.05, we should accept the null hypothesis. So, the assumption of uncorrelated error is valid.

5. Outliers
par(mfrow=c(2,2))
plot(fit)
From Residuals VS Fitted and QQ plot, we further verify the conclusion that there

exist outliers and got the index of them, which are 8,101,179

6.. Influential Points
```{r}
lmi <- lm.influence(fit)
lms <- summary(fit)
e <- resid(fit)
s <- lms$sigma
si <- lmi$sigma
xxi <- diag(lms$cov.unscaled)
h <- lmi$hat
bi <- coef(fit)-t(coef(lmi))
dfbetas <- bi/t(si%o%xxi^0.5)
stand.resid <- e/(si*(1-h)^0.5)
DFFITS <- h^0.5*e/(si*(1-h))
which(abs(stand.resid)>2*sqrt(6/332))
```


**Prob c. The remedial measures in case of violations of any of the underlying assumptions**

1. Nonlinearity:

Simple trasformations, e.g, take log

Non-linear model

Other predictors

2. Non-Normality: 

Transformation

Robust regression methods

3. Influential points: 

Robust regression

